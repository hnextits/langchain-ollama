{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"http://www.infopub.co.kr/new/include/detail.asp?sku=05000274\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0])\n",
    "pprint(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HugoingFace Embeddings를 다운로드\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    ")\n",
    "\n",
    "# HugoingFace Embedding 모델의 Tokenizer를 사용하여 토큰화\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token 수를 기준으ㄹ 문서를 청크 단위로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer = tokenizer,\n",
    "    chunk_size = 120,\n",
    "    chunk_overlap  = 10,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(len(split_docs))\n",
    "print(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=embeddings_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chat Model\n",
    "llm = ChatOllama(model=\"qwen2\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain 실행\n",
    "response = rag_chain.invoke(\"이 책의 특징을 3가지 요점으로 설명해주세요.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data 만들기 - AutoRAG 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorag.data.corpus import langchain_documents_to_parquet\n",
    "corpus_df = langchain_documents_to_parquet(split_docs, 'corpus_data/pandas_book.parquet')\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from autorag.data.qacreation import generate_qa_llama_index, make_single_content_qa\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate question and answer pairs for the given passage.\n",
    "\n",
    "Passage:\n",
    "{{text}}\n",
    "\n",
    "Number of questions to generate: {{num_questions}}\n",
    "\n",
    "Example:\n",
    "[Q]: 이 책의 글쓴이는 누구인가요?\n",
    "[A]: 저자의 이름은 홍길동입니다.\n",
    "\n",
    "Result:\n",
    "\"\"\"\n",
    "\n",
    "corpus_df = pd.read_parquet('corpus_data/pandas_book.parquet')\n",
    "\n",
    "llm = Ollama(model='gemma2', temperature=1.0)\n",
    "qa_df = make_single_content_qa(corpus_df, content_size=36, qa_creation_func=generate_qa_llama_index,\n",
    "                            llm=llm, prompt=prompt, question_num_per_content=1)\n",
    "\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.to_excel('corpus_data/pandas_book_qa.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 수정한 QA 데이터를 불러옴\n",
    "qa_data = pd.read_excel('corpus_data/pandas_book_qa_final.xlsx')\n",
    "qa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(qa_data['retrieval_gt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(qa_data['generation_gt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "for col in ['retrieval_gt', 'generation_gt']:\n",
    "    qa_data[col] = qa_data[col].apply(ast.literal_eval)\n",
    "\n",
    "qa_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(qa_data['retrieval_gt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(qa_data['generation_gt'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS - RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 테스트\n",
    "response = rag_chain.invoke(\"이 책의 출판사는 어디인가요?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = qa_data[['query', 'generation_gt']]\n",
    "qa_data.columns = ['question', 'ground_truth']\n",
    "qa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data['ground_truth'] = qa_data['ground_truth'].apply(lambda x: x[0] )\n",
    "qa_data['answer'] = qa_data['question'].apply(lambda x: rag_chain.invoke(x) )\n",
    "qa_data['contexts'] = qa_data['question'].apply(lambda x: [d.page_content for d in retriever.get_relevant_documents(x)] )\n",
    "qa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "dataset = Dataset.from_pandas(qa_data)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "langchain_llm =  ChatOllama(model=\"gemma2\")\n",
    "langchain_embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ],\n",
    "    llm=langchain_llm, \n",
    "    embeddings=langchain_embeddings,\n",
    "    raise_exceptions=False,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2 모델을 사용한 RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chat Model\n",
    "llm = ChatOllama(model=\"gemma2\")\n",
    "\n",
    "rag_chain2 = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "qa_data2 = qa_data.copy()\n",
    "qa_data2['answer'] = qa_data2['question'].apply(lambda x: rag_chain2.invoke(x) )\n",
    "qa_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = Dataset.from_pandas(qa_data2)\n",
    "\n",
    "result2 = evaluate(\n",
    "    dataset2,\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ],\n",
    "    llm=langchain_llm, \n",
    "    embeddings=langchain_embeddings,\n",
    "    raise_exceptions=False,\n",
    ")\n",
    "\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list(result.items()), columns=['Metric', 'Qwen2'])\n",
    "df2 = pd.DataFrame(list(result2.items()), columns=['Metric', 'Gemma2'])\n",
    "\n",
    "df_result = pd.merge(df1, df2, on='Metric')\n",
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langfuse-practice-ygCFMd-0-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
