{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {},
   "source": [
    "# RAG 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c7374",
   "metadata": {},
   "source": [
    "## Enviornment (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일을 불러옵니다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3208ca",
   "metadata": {},
   "source": [
    "## Evaluation Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20ed79",
   "metadata": {},
   "source": [
    "#### 1) (pydantic) Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64101cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Car(BaseModel):\n",
    "    \"\"\"Information about a car.\"\"\"\n",
    "    make: Optional[str] = Field(default=None, description=\"The make of the car\")\n",
    "    model_name: Optional[str] = Field(default=None, description=\"The model name of the car\")\n",
    "    model_year: Optional[int] = Field(\n",
    "        default=None, description=\"The year the car model was manufactured\"\n",
    "    )\n",
    "    color: Optional[str] = Field(default=None, description=\"The color of the car\")\n",
    "    price: Optional[float] = Field(default=None, description=\"The price of the car\")\n",
    "    mileage: Optional[float] = Field(default=None, description=\"The mileage of the car\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d32df",
   "metadata": {},
   "source": [
    "#### 2) Synthetic Data\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/tutorials/data_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_experimental.tabular_synthetic_data.openai import create_openai_data_generator\n",
    "from langchain_experimental.tabular_synthetic_data.prompts import (\n",
    "    SYNTHETIC_FEW_SHOT_PREFIX,\n",
    "    SYNTHETIC_FEW_SHOT_SUFFIX,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"example\": \"\"\"make: 현대, model_name: 소나타, model_year: 2022, color: 흰색, price: 25000000, mileage: 15000.0\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"example\": \"\"\"make: 기아, model_name: K5, model_year: 2021, color: None, price: 23000000, mileage: 20000.0\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "OPENAI_TEMPLATE = PromptTemplate(input_variables=[\"example\"], template=\"{example}\")\n",
    "\n",
    "prompt_template = FewShotPromptTemplate( \n",
    "    prefix=SYNTHETIC_FEW_SHOT_PREFIX,\n",
    "    examples=examples,\n",
    "    suffix=SYNTHETIC_FEW_SHOT_SUFFIX,\n",
    "    input_variables=[\"subject\", \"extra\"],\n",
    "    example_prompt=OPENAI_TEMPLATE,\n",
    ")\n",
    "\n",
    "synthetic_data_generator = create_openai_data_generator(\n",
    "    output_schema=Car,\n",
    "    llm=ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0.7,\n",
    "    ), \n",
    "    prompt=prompt_template,\n",
    ")\n",
    "\n",
    "synthetic_results = synthetic_data_generator.generate(\n",
    "    subject=\"car data\",\n",
    "    extra=\"Use Korean language. Make it something you wouldn't normally choose. Around 30 percent of the values should be None at random. \",\n",
    "    runs=10,\n",
    ")\n",
    "\n",
    "len(synthetic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHETIC_FEW_SHOT_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHETIC_FEW_SHOT_SUFFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "car_dicts = [car.dict() for car in synthetic_results]\n",
    "\n",
    "df = pd.DataFrame(car_dicts)\n",
    "df.to_csv(\"car_data.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b830a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.synthetic_data import DatasetGenerator\n",
    "\n",
    "# Dataset Generator\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.7)\n",
    "generator = DatasetGenerator(model, {\"style\": \"informal\", \"minimal length\": 300, \"language\": \"Korean\"})\n",
    "dataset = generator(synthetic_results)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model2 = ChatOllama(model='qwen2', temperature=0.7)\n",
    "generator2 = DatasetGenerator(model2, {\"style\": \"informal\", \"minimal length\": 300, \"language\": \"Korean\"})\n",
    "dataset2 = generator2(synthetic_results)\n",
    "\n",
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b66730",
   "metadata": {},
   "source": [
    "## Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594015b6",
   "metadata": {},
   "source": [
    "#### 1) Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a2fbe",
   "metadata": {},
   "source": [
    "#### 2) OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "extract_chain = prompt | llm.with_structured_output(schema=Car)\n",
    "\n",
    "extract_result = extract_chain.invoke({\"text\": dataset[0]['text']})\n",
    "\n",
    "extract_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d43426",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8221ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_result == dataset[0]['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9144327",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_result = extract_chain.invoke({\"text\": dataset[1]['text']})\n",
    "\n",
    "extract_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_result == dataset[1]['fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2c020",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad245c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are an expert extraction algorithm. Only extract relevant information from the text.\n",
    "If you do not know the value of an attribute asked to extract, return null for the attribute's value.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "TEXT: {text}\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = OllamaFunctions(model=\"llama3\", format=\"json\")\n",
    "\n",
    "extract_chain = prompt | llm.with_structured_output(schema=Car)\n",
    "\n",
    "extract_result = extract_chain.invoke({\"text\": dataset[0]['text']})\n",
    "\n",
    "extract_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_result == dataset[0]['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b812a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d93254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
